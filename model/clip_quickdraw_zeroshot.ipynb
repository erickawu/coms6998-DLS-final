{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S9H0k1fJz9Ls"
   },
   "outputs": [],
   "source": [
    "# unpack data from npy files into folders (trial of 5000 samples per 10 classes)\n",
    "# in total, quickdraw has 345 classes\n",
    "root = \"../\"\n",
    "img_array = np.load(root + 'data/apple.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 2731,
     "status": "ok",
     "timestamp": 1618805382522,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "3SDCpMEb0EFm",
    "outputId": "b6758770-4738-48ca-b4ec-614e7cf1846c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reformat data and create tokenizations for CLIP\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(img_array[0].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hpJQCUz2FuB"
   },
   "source": [
    "CLIP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3284,
     "status": "ok",
     "timestamp": 1618809050300,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "8OT91VVD2APo",
    "outputId": "f85e62db-4c4d-456f-c898-e6831a29b559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 11.0\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.7.1+cu110 in /opt/conda/lib/python3.7/site-packages (1.7.1+cu110)\n",
      "Requirement already satisfied: torchvision==0.8.2+cu110 in /opt/conda/lib/python3.7/site-packages (0.8.2+cu110)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (6.0.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (2021.3.17)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1+cu110) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.8.2+cu110) (8.1.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy) (0.2.5)\n",
      "Torch version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# installation dependencies (run this first, then restart kernel)\n",
    "import subprocess\n",
    "\n",
    "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
    "print(\"CUDA version:\", CUDA_version)\n",
    "\n",
    "if CUDA_version == \"10.0\":\n",
    "    torch_version_suffix = \"+cu100\"\n",
    "elif CUDA_version == \"10.1\":\n",
    "    torch_version_suffix = \"+cu101\"\n",
    "elif CUDA_version == \"10.2\":\n",
    "    torch_version_suffix = \"\"\n",
    "else:\n",
    "    torch_version_suffix = \"+cu110\"\n",
    "\n",
    "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1618805371834,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "45-pBpAQ2JJ9",
    "outputId": "03b8a669-adb8-4dce-c142-126aafd47a08"
   },
   "outputs": [],
   "source": [
    "# clone\n",
    "!git clone https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (6.0.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1618809056051,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "SsHPTLmt2Nkl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from CLIP import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 1753,
     "status": "ok",
     "timestamp": 1618809061751,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "0lK8eIuL5eWZ"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ViT-B/32 works with 224x224 img \n",
    "test_model, transform = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1618809064241,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "lC3I5R1O7KZh"
   },
   "outputs": [],
   "source": [
    "# get list of class names\n",
    "class_names = []\n",
    "with open(root + 'data/100categories.txt') as f:\n",
    "    class_names = f.read().splitlines()\n",
    "target_names = [\"a sketch of \" + cls for cls in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 6450,
     "status": "error",
     "timestamp": 1618809087885,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "cmL-6V_z6TyI",
    "outputId": "7f327c82-dcbc-4d73-c660-d416fb5c74ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21.8750, 19.4688, 22.4219, 19.0312, 21.5469, 21.8906, 23.7031, 20.6719,\n",
      "         20.7656, 21.3594, 21.8125, 23.8438, 21.6719, 22.0312, 22.0312, 21.2969,\n",
      "         22.9375, 19.4688, 22.1250, 22.3125, 21.9844, 21.5000, 20.9219, 20.0000,\n",
      "         20.0156, 23.0781, 20.8281, 19.3750, 18.1250, 20.4844, 20.7344, 20.9688,\n",
      "         21.9062, 22.2031, 20.1250, 23.4062, 20.3906, 20.7344, 20.6094, 20.7344,\n",
      "         24.8125, 19.8281, 19.8594, 22.9375, 20.5938, 21.3125, 20.6562, 20.8438,\n",
      "         20.0000, 19.8281, 21.6875, 19.4844, 20.4375, 21.2969, 21.7031, 20.1094,\n",
      "         21.1094, 20.3906, 20.4688, 20.5469, 22.1094, 21.3906, 21.3906, 20.5000,\n",
      "         21.2500, 22.9375, 24.8594, 20.0938, 21.9219, 21.3125, 20.5469, 22.3594,\n",
      "         20.3594, 22.2188, 20.9219, 21.5469, 20.2031, 22.9375, 22.5781, 20.0781,\n",
      "         19.4531, 20.0469, 21.1250, 20.7969, 22.6250, 20.3281, 20.8750, 20.6094,\n",
      "         20.1250, 20.8438, 20.3750, 20.5938, 21.6719, 20.8125, 18.9375, 20.9375,\n",
      "         19.1094, 21.7812, 19.8125, 21.7031]], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4f105629feb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# text_features = test_model.encode_text(class_text_tokens).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# get probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mlogits_per_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_per_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_text_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# attempt to classify and look @ accuracy\n",
    "correct = []\n",
    "\n",
    "def argmax(iterable):\n",
    "    return max(enumerate(iterable), key=lambda x: x[1])[0]\n",
    "\n",
    "# define target text classifications\n",
    "class_text_tokens = clip.tokenize(target_names).to(device)\n",
    "\n",
    "for cls in class_names:\n",
    "    class_correct = []\n",
    "    class_img_array = np.load(root + 'data/' + cls + '.npy')\n",
    "    for img in class_img_array[:50]:\n",
    "        pil_img = Image.fromarray(img.reshape((28,28)), 'L')\n",
    "        image = transform(pil_img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "          # encode image and then encode text\n",
    "            # image_features = test_model.encode_image(image).to(device)\n",
    "            # text_features = test_model.encode_text(class_text_tokens).to(device)\n",
    "            # get probs\n",
    "            logits_per_image, logits_per_text = test_model(image, class_text_tokens)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "            # make prediction \n",
    "            pred = class_names[argmax(list(probs)[0])]\n",
    "            if pred == cls:\n",
    "                correct.append(1)\n",
    "                class_correct.append(1)\n",
    "            else:\n",
    "                correct.append(0)\n",
    "                class_correct.append(0)\n",
    "    \n",
    "    print('accuracy on class ' + cls + ' is: ' + str(sum(class_correct)/len(class_correct)))\n",
    "print('accuracy on all is: ' + str(sum(correct)/len(correct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1618805703849,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "9Q6QYW1pVgPr",
    "outputId": "15602170-8de6-48ac-8e21-32d2f0b62b73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49406,   320,  5269,   539, 16637, 49407,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(class_text_tokens[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIMa6ZZYbOcT"
   },
   "source": [
    "Fine-Tuning CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1618809091608,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "0fcGGnu4G5D6"
   },
   "outputs": [],
   "source": [
    "# functions to load in data and construct pytorch dataloader \n",
    "def load_dataset(root, mtype):\n",
    "    num_classes = 100\n",
    "\n",
    "    if os.path.exists(os.path.join(root, mtype+'.npz')):\n",
    "        print(\"*\"*50)\n",
    "        print(\"Loading \"+mtype+\" dataset...\")\n",
    "        print(\"*\"*50)\n",
    "        print(\"Classes number of \"+mtype+\" dataset: \"+str(num_classes))\n",
    "        print(\"*\"*50)\n",
    "        data_cache = np.load(os.path.join(root, mtype+'.npz'))\n",
    "        return data_cache[\"data\"].astype('float32'), \\\n",
    "            data_cache[\"target\"].astype('int64'), num_classes\n",
    "    \n",
    "    else:\n",
    "        raise FileNotFoundError(\"%s doesn't exist!\" %\n",
    "                                os.path.join(root, mtype+'.npz'))\n",
    "\n",
    "class QD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mtype, root):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - mytpe: str, specify the type of the dataset, i.e, 'train' or 'test'\n",
    "        - root: str, specify the root of the dataset directory\n",
    "        \"\"\"\n",
    "        self.data, self.target, self.num_classes = load_dataset(root, mtype)\n",
    "        print(\"dataset \"+mtype+\" loading done.\")\n",
    "        print(\"*\"*50+\"\\n\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        curr = self.data[index]\n",
    "        pil_img = Image.fromarray(curr.reshape((28,28)), 'L')\n",
    "        pil_img = transform(pil_img)\n",
    "        caption = \"a sketch of \" + class_names[self.target[index]]\n",
    "        return pil_img, caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_number_classes(self):\n",
    "        return self.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9475,
     "status": "ok",
     "timestamp": 1618809102831,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "nfaRX2k_Zt78",
    "outputId": "d7ef1948-d6bc-485d-8a4c-a5df2d36bed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Loading train dataset...\n",
      "**************************************************\n",
      "Classes number of train dataset: 100\n",
      "**************************************************\n",
      "dataset train loading done.\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = QD_Dataset(mtype = \"train\", root=root+\"data/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 8824,
     "status": "ok",
     "timestamp": 1618809102832,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "w8cXgeRbageR"
   },
   "outputs": [],
   "source": [
    "# 400,000 images (400 per class in training) (the other 100 in test!)\n",
    "# class_names array maps from index to class name\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 12783,
     "status": "ok",
     "timestamp": 1618809107392,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "1ruDUIU6WVde"
   },
   "outputs": [],
   "source": [
    "test_model, transform = clip.load(\"ViT-B/32\", device='cuda', jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1618809271605,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "puc2wXdvUrfx"
   },
   "outputs": [],
   "source": [
    "# freeze first n layers\n",
    "def freeze(model, num_layers_to_freeze):\n",
    "  ct = 0\n",
    "  for child in model.children():\n",
    "    ct += 1\n",
    "    if ct < num_layers_to_freeze:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# train function\n",
    "def finetune_CLIP(model, train_loader, num_epochs):\n",
    "  model.train()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params from paper\n",
    "  loss_img = torch.nn.CrossEntropyLoss()\n",
    "  loss_txt = torch.nn.CrossEntropyLoss()\n",
    "  loss_epoch = 0\n",
    "  for e in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      # get img and text features\n",
    "      images = torch.stack([img for img in inputs], dim=0).to(device)\n",
    "      text_features = clip.tokenize(labels).to(device)\n",
    "      # get logit outputs\n",
    "      logits_per_image, logits_per_text = model(images, text_features)\n",
    "      # get ground truth\n",
    "      ground_truth = torch.arange(inputs.size(0)).long().to(device)\n",
    "      # compute loss\n",
    "      total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "      loss_epoch += total_loss.item()\n",
    "      total_loss.backward()\n",
    "      optimizer.step()\n",
    "      if ((i+1) % 100) == 0:\n",
    "        print(total_loss.item())\n",
    "    print(\"Epoch %d, Loss: %f\" % (e+1, loss_epoch/len(trainloader)))\n",
    "    # reset loss every epoch\n",
    "    loss_epoch = 0\n",
    "  print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXDwWHghb-3V"
   },
   "outputs": [],
   "source": [
    "# freeze language transformer?\n",
    "for k in test_model.transformer.parameters():  \n",
    "  k.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1618809184082,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "5FqGCjlKi9Ka"
   },
   "outputs": [],
   "source": [
    "def argmax(iterable):\n",
    "    return max(enumerate(iterable), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 386201,
     "status": "error",
     "timestamp": 1618809662860,
     "user": {
      "displayName": "Ericka Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh16kwJgWVswXGZeTRdmSbxPZth2-yB0bdCDr4R=s64",
      "userId": "11564995692578738679"
     },
     "user_tz": 240
    },
    "id": "FgY9-jfycZjD",
    "outputId": "da490561-e5e1-47f8-9ce5-8cc9792bec43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.16015625\n",
      "4.16015625\n",
      "4.16015625\n",
      "4.17578125\n",
      "4.15625\n",
      "4.15625\n",
      "4.16015625\n",
      "4.16015625\n",
      "4.15625\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-573f1b419c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinetune_CLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-13029e7ca59e>\u001b[0m in \u001b[0;36mfinetune_CLIP\u001b[0;34m(model, train_loader, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mloss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.40 GiB already allocated; 19.75 MiB free; 13.69 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "finetune_CLIP(test_model, train_loader, 5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMf4Mp2IWPQiTQyEOsI7hvu",
   "name": "clip_quickdraw_zeroshot.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
